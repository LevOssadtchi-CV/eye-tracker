# eye-tracker

**eye-tracker** — это легковесный проект для отслеживания взгляда, построенный на основе **MediaPipe** и **PyTorch**. Он захватывает видео с веб-камеры, определяет область глаз и использует обученную сверточную нейронную сеть (CNN), чтобы в реальном времени предсказывать точку взгляда на экране. Этот проект идеально подходит для прототипирования интерфейсов, управляемых взглядом, проведения экспериментов и исследований в области взаимодействия человека и компьютера.

## Особенности

-   **Отслеживание в реальном времени**: использует модель глубокого обучения для быстрого и точного определения точки взгляда.
-   **MediaPipe Integration**: применяет `MediaPipe Face Mesh` для эффективного и надёжного обнаружения глаз и получения ключевых точек.
-   **Модульная архитектура**: пайплайн разделён на отдельные скрипты для сбора данных, подготовки датасета, обучения модели и тестирования.
-   **Две модели**: включает в себя базовую (**`SimpleGazeNet`**) и более сложную (**`ComplexGazeNet`**) модели], что позволяет выбирать между производительностью и точностью.
-   **Калибровка**: скрипты для тестирования включают функцию калибровки, которая корректирует предсказания модели, повышая их точность.
-   **Сглаживание предсказаний**: `test_model2.py` использует экспоненциальное сглаживание для более плавной и стабильной траектории точки взгляда.

---

## Базовый пайплайн работы

Рабочий процесс проекта состоит из нескольких ключевых шагов: от сбора данных до запуска финальной модели. Каждый шаг выполняется отдельным скриптом.

0. * Проверьте правильность захвата глаз с помощью **`test.py`**
     
1.  **Сбор данных**:
    * Скрипт: **`make_dataset.py`**
    * Описание: Скрипт использует вашу веб-камеру для захвата видео. Он отображает на экране движущуюся точку, которая служит целью для взгляда. На каждом кадре он определяет область глаз с помощью MediaPipe, кадрирует её и сохраняет изображение, а также относительные координаты точки взгляда в CSV-файл.

2.  **Подготовка датасета**:
    * Скрипт: **`split.py`**
    * Описание: Этот скрипт берёт все собранные изображения и CSV-файл и разделяет их на три части: для обучения (`train`), валидации (`val`) и тестирования (`test`). Он создаёт соответствующие папки и сохраняет отдельные метаданные для каждой части.

3.  **Обучение модели**:
    * Скрипт: **`model.py`** (для более сложной модели) или **`baseline.py`** (для базовой модели).
    * Описание: Выберите одну из двух моделей. Скрипт обучает CNN на подготовленных данных, используя набор для обучения. В процессе обучения он проверяет модель на валидационном наборе и сохраняет веса лучшей модели, что помогает избежать переобучения.

4.  **Тестирование и запуск в реальном времени**:
    * Скрипт: **`test_model.py`** или **`test_model2.py`**.
    * Описание: Эти скрипты загружают обученную модель. Они захватывают видео с веб-камеры, определяют область глаз и используют модель, чтобы предсказать, куда смотрит пользователь. Проект может отображать предсказанную точку прямо на видеопотоке (`test_model.py`) или на отдельном чёрном холсте (`test_model2.py`), что имитирует полноэкранный режим. В любой момент можно провести быструю калибровку, чтобы скорректировать смещение предсказаний.
